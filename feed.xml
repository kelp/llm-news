<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>anthropic-feed</id>
  <title>Anthropic News and Research</title>
  <updated>2025-06-07T12:01:37.931130+00:00</updated>
  <author>
    <name>Anthropic Feed Generator</name>
  </author>
  <link href="https://tcole.net/llm-news/feed.atom" rel="self"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-raises-124-million-to-build-more-reliable-general-ai-systems</id>
    <title>Anthropic raises $124 million to build more reliable, general AI systems</title>
    <updated>2021-05-28T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Anthropic, an AI safety and research company, has raised $124 million in a Series A. The financing round will support Anthropic in executing against its research roadmap and building prototypes of reliable and steerable AI systems.The company is led by siblings Dario Amodei (CEO) and Daniela Amodei (President). The Anthropic team has previously conducted research intoGPT-3,Circuit-Based Interpretability,Multimodal Neurons,Scaling Laws,AI &amp; Compute,Concrete Problems in AI Safety, andLearning from Human Preferences. Anthropic will use the funding for computationally-intensive research to develop large-scale AI systems that are steerable, interpretable, and robust.“Anthropic’s goal is to make the fundamental research advances that will let us build more capable, general, and reliable AI systems, then deploy these systems in a way that benefits people. We’re thrilled to be working with investors that support us in this mission and expect to concentrate on research in the immediate term,” said Anthropic CEO Dario Amodei.Anthropic will focus on research into increasing the safety of AI systems; specifically, the company is focusing on increasing the reliability of large-scale AI models, developing the techniques and tools to make them more interpretable, and building ways to more tightly integrate human feedback into the development and deployment of these systems.The Series A round was led by Jaan Tallinn, technology investor and co-founder of Skype. The round included participation from James McClave, Dustin Moskovitz, the Center for Emerging Risk Research, Eric Schmidt, and others.To find out more about Anthropic’s research agenda and approach, you can read our website and its job postings. The company is hiring researchers, engineers, and operational experts to support it in executing against its research roadmap. Find out more here: Anthropic.com.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-raises-124-million-to-build-more-reliable-general-ai-systems"/>
    <category term="news" label="News"/>
    <published>2021-05-28T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-raises-series-b-to-build-safe-reliable-ai</id>
    <title>Anthropic Raises Series B to Build Steerable, Interpretable, Robust AI Systems</title>
    <updated>2022-04-29T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Anthropic, an AI safety and research company, has raised $580 million in a Series B. The financing will help Anthropic build large-scale experimental infrastructure to explore and improve the safety properties of computationally intensive AI models.Since its founding at the beginning of 2021, Anthropic has conducted research into making systems that are more steerable, robust, and interpretable. On interpretability, it has made progress in mathematicallyreverse engineering the behavior of small language modelsand begun to understand the source ofpattern-matching behavior in large language models. On steerability and robustness, it has developedbaseline techniques to make large language models more “helpful and harmless”, and followed this up withreinforcement learning to further improve these properties, as well as releasing a dataset to help other research labs train models that are morealigned with human preferences. It has also released an analysis ofsudden changes in performance in large language models and the societal impacts of this phenomenon, which demonstrates the need for studying safety issues at scale.The purpose of this research is to develop the technical components necessary to build large-scale models which have better implicit safeguards and require less after-training interventions, as well as to develop the tools necessary to further look inside these models to be confident that the safeguards actually work. The company is also building out teams and partnerships dedicated to exploring the policy and societal impacts of these models.“With this fundraise, we’re going to explore the predictable scaling properties of machine learning systems, while closely examining the unpredictable ways in which capabilities and safety issues can emerge at-scale,” said Anthropic co-founder and CEO Dario Amodei. “We’ve made strong initial progress on understanding and steering the behavior of AI systems, and are gradually assembling the pieces needed to make usable, integrated AI systems that benefit society.”Anthropic is now a growing team of around 40 people based in a plant-filled office in San Francisco, California, with plans to expand further this year. “Now that we’ve built out the organization, we’re focusing on ensuring Anthropic has the culture and governance to continue to responsibly explore and develop safe AI systems as we scale,” said Anthropic co-founder and President Daniela Amodei. “We’re excited about what’s ahead, and grateful to all be working together.”The Series B follows the company raising $124 million in a Series A round in 2021. The Series B round was led by Sam Bankman-Fried, CEO of FTX. The round also included participation from Caroline Ellison, Jim McClave, Nishad Singh, Jaan Tallinn, and the Center for Emerging Risk Research (CERR).&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-raises-series-b-to-build-safe-reliable-ai"/>
    <category term="news" label="News"/>
    <published>2022-04-29T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-partners-with-google-cloud</id>
    <title>Anthropic Partners with Google Cloud</title>
    <updated>2023-02-03T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Anthropic, an AI safety and research company, hasselectedGoogle Cloud as its cloud provider. The partnership is designed so that the companies can co-develop AI computing systems; Anthropic will leverage Google Cloud's cutting-edge GPU and TPU clusters to train, scale, and deploy its AI systems.“We're partnering with Google Cloud to support the next phase of Anthropic, where we're going to deploy our AI systems to a larger set of people,” said Anthropic CEO Dario Amodei. “This partnership gives us the cloud infrastructure performance and scale we need.”Anthropic is focused on developing and deploying Claude, an AI assistant based on the company's research into building safe, steerable AI. Anthropic has created safety techniques likeConstitutional AIto create AI technologies that are easier to rely on and understand.“We are eager to use the Google Cloud infrastructure to build reliable, interpretable, and steerable AI systems. This partnership with Google Cloud will let us build a more robust AI platform,” said Dario Amodei.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-partners-with-google-cloud"/>
    <category term="news" label="News"/>
    <published>2023-02-03T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/core-views-on-ai-safety</id>
    <title>Core Views on AI Safety: When, Why, What, and How</title>
    <updated>2023-03-08T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We founded Anthropic because we believe the impact of AI might be comparable to that of the industrial and scientific revolutions, but we aren’t confident it will go well. And we also believe this level of impact could start to arrive soon – perhaps in the coming decade.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/core-views-on-ai-safety"/>
    <category term="news" label="News"/>
    <published>2023-03-08T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/introducing-claude</id>
    <title>Introducing Claude</title>
    <updated>2023-03-14T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;After working for the past few months with key partners like Notion, Quora, and DuckDuckGo in a closed alpha, we’ve been able to carefully test out our systems in the wild, and areready to offer Claude more broadlyso it can power crucial, cutting-edge use cases at scale.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/introducing-claude"/>
    <category term="news" label="News"/>
    <published>2023-03-14T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-now-in-slack</id>
    <title>Claude, now in Slack</title>
    <updated>2023-03-30T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we are releasing the newClaude App for Slack, now in beta. Built on top of Slack’s platform, the app can summarize threads, answer questions, and more. Now any company has the chance to have a “virtual teammate” who can help make work more fun and productive.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-now-in-slack"/>
    <category term="news" label="News"/>
    <published>2023-03-30T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/an-ai-policy-tool-for-today-ambitiously-invest-in-nist</id>
    <title>An AI Policy Tool for Today: Ambitiously Invest in NIST</title>
    <updated>2023-04-20T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We believe that sensible artificial intelligence (AI) policy requires, among other things, the ability to accurately describe and quantify the capabilities and risks of AI systems. This ability is both an enabler and a prerequisite to effective regulation, as measurement tools allow us to objectively assess systems and ensure they meet appropriate safety thresholds. In this post, we propose a policy intervention for how to do this: ambitiously fund the National Institute of Standards and Technology (NIST) to support its AI measurement and standards efforts.We were heartened by the bipartisan support for maintaining American leadership in the development of critical technologies, as expressed during the April 18budget hearingon the 2024 Request for the Department of Commerce (and by extension, NIST). We think one of the best ways to channel that support is through an increase in federal funding for NIST so that it is well placed to carry out its work promoting safe technological innovation.In this post, we give an overview of why we think this, and we also share a policy proposal for what an ambitious funding program for NIST could look like in practice. This proposal is readily actionable and builds on a solid foundation of existing work at the agency; we view it as a complement to a suite of policy levers for stronger AI governance.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/an-ai-policy-tool-for-today-ambitiously-invest-in-nist"/>
    <category term="news" label="News"/>
    <published>2023-04-20T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/partnering-with-scale</id>
    <title>Partnering with Scale to Bring Generative AI to Enterprises</title>
    <updated>2023-04-26T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are pleased to announce our partnership withScale, a leading platform for building, deploying and managing Generative AI applications. Scale customers will now be able to use Claude, our conversational AI assistant based on research into training helpful, honest, and harmless systems.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/partnering-with-scale"/>
    <category term="news" label="News"/>
    <published>2023-04-26T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claudes-constitution</id>
    <title>Claude’s Constitution</title>
    <updated>2023-05-09T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;How does a language model decide which questions it will engage with and which it deems inappropriate? Why will it encourage some actions and discourage others? What “values” might a language model have?These are all questions people grapple with. Our recently published research on “Constitutional AI” provides one answer by giving language models explicit values determined by a constitution, rather than values determined implicitly via large-scale human feedback. This isn’t a perfect approach, but it does make the values of the AI system easier to understand and easier to adjust as needed.Since launchingClaude, our AI assistant trained with Constitutional AI, we've heard more questions about Constitutional AI and how it contributes to making Claude safer and more helpful. In this post, we explain what constitutional AI is, what the values in Claude’s constitution are, and how we chose them.If you just want to skip to the principles, scroll down to the last section which is entitled “The Principles in Full.”&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claudes-constitution"/>
    <category term="news" label="News"/>
    <published>2023-05-09T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/100k-context-windows</id>
    <title>Introducing 100K Context Windows</title>
    <updated>2023-05-11T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We’ve expanded Claude’s context window from 9K to 100K tokens, corresponding to around 75,000 words! This means businesses can now submithundreds of pagesof materials for Claude to digest and analyze, and conversations with Claude can go on for hours or even days.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/100k-context-windows"/>
    <category term="news" label="News"/>
    <published>2023-05-11T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/zoom-partnership-and-investment</id>
    <title>Zoom Partnership and Investment in Anthropic</title>
    <updated>2023-05-16T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are announcing a new partnership with Zoom, a leader in enterprise collaboration and communication solutions. Zoom will use Claude, our AI assistant built with Constitutional AI, to build customer-facing AI products focused on reliability, productivity, and safety.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/zoom-partnership-and-investment"/>
    <category term="news" label="News"/>
    <published>2023-05-16T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-series-c</id>
    <title>Anthropic Raises $450 Million in Series C Funding to Scale Reliable AI Products</title>
    <updated>2023-05-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are pleased to announce that we have raised $450 million in Series C funding led by Spark Capital with participation from Google, Salesforce Ventures, Sound Ventures, Zoom Ventures, and others. The funding will support our continued work developing helpful, harmless, and honest AI systems—including Claude, an AI assistant that can perform a wide variety of conversational and text processing tasks.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-series-c"/>
    <category term="news" label="News"/>
    <published>2023-05-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/charting-a-path-to-ai-accountability</id>
    <title>Charting a Path to AI Accountability</title>
    <updated>2023-06-13T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;This week, Anthropic submitteda responseto the National Telecommunications and Information Administration’s (NTIA)Request for Comment on AI Accountability. Today, we want to share our recommendations as they capture some of Anthropic’s core AI policy proposals.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/charting-a-path-to-ai-accountability"/>
    <category term="news" label="News"/>
    <published>2023-06-13T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-2</id>
    <title>Claude 2</title>
    <updated>2023-07-11T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are pleased to announceClaude 2, our new model. Claude 2 has improved performance, longer responses, and can be accessed via API as well as a new public-facing beta website,claude.ai. We have heard from our users that Claude is easy to converse with, clearly explains its thinking, is less likely to produce harmful outputs, and has a longer memory. We have made improvements from our previous models on coding, math, and reasoning. For example, our latest model scored76.5%on the multiple choice section of the Bar exam, up from 73.0% with Claude 1.3. When compared to college students applying to graduate school, Claude 2 scores above the 90th percentile on the GRE reading and writing exams, and similarly to the median applicant on quantitative reasoning.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-2"/>
    <category term="news" label="News"/>
    <published>2023-07-11T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/frontier-model-security</id>
    <title>Frontier Model Security</title>
    <updated>2023-07-25T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;As the capabilities of frontier artificial intelligence models continue to increase rapidly, ensuring the security of these systems has become a critical priority. In our previous posts, we’ve focused on Anthropic’sapproach to safety, and Claude’s capabilities and applications. In this post, we are sharing some of the steps we are taking to ensure our models are developed securely. We hope to advance public discussion about how all labs can deploy top models securely, as well as share recommendations for government regulatory approaches that encourage adoption of strong cybersecurity practices. Below we discuss some of our recommendations for cybersecurity best practices, which Anthropic itself is in the process of implementing.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/frontier-model-security"/>
    <category term="news" label="News"/>
    <published>2023-07-25T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety</id>
    <title>Frontier Threats Red Teaming for AI Safety</title>
    <updated>2023-07-26T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;“Red teaming,” or adversarial testing, is a recognized technique to measure and increase the safety and security of systems. Whileprevious Anthropic researchreported methods and results for red teaming using crowdworkers, for some time, AI researchers have noted that AI models could eventually obtain capabilities in areas relevant to national security. For example, researchers havecalledtomeasureandmonitorthese risks, and havewrittenpaperswith evidence of risks. Anthropic CEO Dario Amodei also highlighted this topic inrecent Senate testimony. With that context, we were pleased to advocate for and join in commitments announced at the White House on July 21 that included “internal and external security testing of [our] AI systems” to guard against “some of the most significant sources of AI risks, such as biosecurity and cybersecurity.” However, red teaming in these specialized areas requires intensive investments of time and subject matter expertise.In this post, we share our approach to “frontier threats red teaming,” high level findings from a project we conducted on biological risks as a test project, lessons learned, and our future plans in this area.Our goal in this work is to evaluate a baseline of risk, and to create a repeatable way to perform frontier threats red teaming across many topic areas. With respect to biology, while the details of our findings are highly sensitive, we believe it’s important to share our takeaways from this work. In summary, working withexperts, we found that models might soon present risks to national security, if unmitigated. However, we also found that there are mitigations to substantially reduce these risks.We are nowscaling up this workin order to reliably identify risks and build mitigations. We believe that improving frontier threats red teaming will have immediate benefits and contribute tolong-term AI safety. We have been sharing our findings with government, labs, and other stakeholders, and we’d like to see more independent groups doing this work.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety"/>
    <category term="news" label="News"/>
    <published>2023-07-26T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/releasing-claude-instant-1-2</id>
    <title>Releasing Claude Instant 1.2</title>
    <updated>2023-08-09T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Businesses working with Claude can now access our latest version of Claude Instant, version 1.2, availablethrough our API. Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest model Claude 2 in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/releasing-claude-instant-1-2"/>
    <category term="news" label="News"/>
    <published>2023-08-09T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/skt-partnership-announcement</id>
    <title>SKT Partnership Announcement</title>
    <updated>2023-08-15T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are pleased to announce that SK Telecom ("SKT"), the largest mobile operator in Korea rapidly integrating AI into its business, has become a commercial partner with Anthropic as well as a strategic investor.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/skt-partnership-announcement"/>
    <category term="news" label="News"/>
    <published>2023-08-15T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-2-amazon-bedrock</id>
    <title>Claude 2 on Amazon Bedrock</title>
    <updated>2023-08-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We’re excited that Claude 2 is now available to customers on Amazon Bedrock.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-2-amazon-bedrock"/>
    <category term="news" label="News"/>
    <published>2023-08-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-pro</id>
    <title>Introducing Claude Pro</title>
    <updated>2023-09-07T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re introducing a paid plan for ourClaude.aichat experience, currently available in the US and UK.Since launching in July, users tell us they’ve chosen Claude.ai as their day-to-day AI assistant for its longer context windows, faster outputs, complex reasoning capabilities, and more. Many also shared that they would value more file uploads and conversations over longer periods.WithClaude Pro, subscribers can now gain5x more usageof our latest model, Claude 2, for a monthly price of $20 (US) or £18 (UK).This means you can level up your productivity across a range of tasks, including summarizing research papers, querying contracts, and iterating further on coding projects—like this recentdemoof building an interactive map.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-pro"/>
    <category term="news" label="News"/>
    <published>2023-09-07T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-bcg</id>
    <title>Anthropic partners with BCG</title>
    <updated>2023-09-14T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We’re pleased to announce our new collaboration with Boston Consulting Group (BCG) to bring Claude to more enterprises. BCG customers around the world will get direct access to our AI assistant to power their strategic AI offerings and deploy safer, more reliable AI solutions.Our work towards creating helpful, honest and harmless systems with techniques likeConstitutional AIaligns with BCG’s focus onresponsible AI. Through this collaboration, BCG will advise their customers on strategic applications of AI and help them deploy Anthropic models includingClaude 2to deliver business results. Use cases involving Claude span knowledge management, market research, fraud detection, demand forecasting, report generation, business analysis and more.Anthropic and BCG have already partnered to help organizations understand the force-multiplying impact of generative AI, most recently at the United Nations. In addition to working together to bring AI to new organizations, BCG has partnered with Anthropic to use Claude within its own teams. We're excited to see how Claude will provide BCG with the ability to synthesize research effectively, analyze data quickly, and drive inspired insights to clients.“The large enterprises I talk with are focused on harnessing value and bottom line impact from AI, and doing that in the most effective and ethical way possible. Aligning these two aspects of AI is a challenge and the price for getting it wrong can be immense, both financially and in reputational harm. Our new collaboration with Anthropic will help deliver that alignment on ethics and effective GenAI,” says Sylvain Duranton, global leader of BCG X, BCG’s tech build and design unit. “Together, we aim to set a new standard for responsible enterprise AI and promote a safety race to the top for AI to be deployed ethically.”We’re extending a warm welcome to BCG and its customers—and look forward to working with them to deploy innovative applications of generative AI safely and responsibly.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-bcg"/>
    <category term="news" label="News"/>
    <published>2023-09-14T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/the-long-term-benefit-trust</id>
    <title>The Long-Term Benefit Trust</title>
    <updated>2023-09-19T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we are sharing more details about our new governance structure called theLong-Term Benefit Trust (LTBT), which we have been developing since the birth of Anthropic. The LTBT is our attempt to fine-tune our corporate governance to address the unique challenges and long-term opportunities we believetransformative AI will present.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/the-long-term-benefit-trust"/>
    <category term="news" label="News"/>
    <published>2023-09-19T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropics-responsible-scaling-policy</id>
    <title>Anthropic's Responsible Scaling Policy</title>
    <updated>2023-09-19T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re publishing ourResponsible Scaling Policy (RSP)– a series of technical and organizational protocols that we’re adopting to help us manage the risks of developing increasingly capable AI systems.As AI models become more capable, we believe that they will create major economic and social value, but will also present increasingly severe risks. Our RSP focuses on catastrophic risks – those where an AI model directly causes large scale devastation. Such risks can come from deliberate misuse of models (for example use by terrorists or state actors to create bioweapons) or from models that cause destruction by acting autonomously in ways contrary to the intent of their designers.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"/>
    <category term="news" label="News"/>
    <published>2023-09-19T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/prompting-long-context</id>
    <title>Prompt engineering for Claude's long context window</title>
    <updated>2023-09-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Claude’s100,000 token long context windowenables the model to operate over hundreds of pages of technical documentation, or even anentire book. As we continue to scale the Claude API, we’re seeing increased demand for prompting guidance on how to maximize Claude’s potential. Today, we’re pleased to share a quantitative case study on two techniques that can improve Claude’s recall over long contexts:&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/prompting-long-context"/>
    <category term="news" label="News"/>
    <published>2023-09-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-amazon</id>
    <title>Expanding access to safer AI with Amazon</title>
    <updated>2023-09-25T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re announcing that Amazon will invest up to $4 billion in Anthropic. The agreement is part of a broader collaboration to develop the most reliable and high-performing foundation models in the industry. Our frontier safety research and products, together with Amazon Web Services’ (AWS) expertise in running secure, reliable infrastructure, will make Anthropic’s safe and steerable AI widely accessible to AWS customers.AWS will become Anthropic’s primary cloud provider for mission critical workloads, providing our team with access to leading compute infrastructure in the form of AWS Trainium and Inferentia chips, which will be used in addition to existing solutions for model training and deployment. Together, we’ll combine our respective expertise to collaborate on the development of future Trainium and Inferentia technology.Based on significant AWS customer demand for Claude, we’re also expanding our support ofAmazon Bedrock. This will include secure model customization and fine-tuning on the service to enable enterprises to optimize Claude’s performance with their expert knowledge, while limiting the potential for harmful outcomes.Amazon developers and engineers will be able to build on top of our state-of-the-art models via Amazon Bedrock. This will enable them to incorporate generative AI capabilities into their work, enhance existing applications, and create net-new customer experiences across Amazon’s businesses.Organizations will be able to useClaude 2for a wide range of tasks, from sophisticated dialogue and creative content generation to complex reasoning and detailed instruction. Our industry-leading 100,000 token context window will help them securely process extensive amounts of information—including technical, domain-specific documents for use cases across finance, legal, coding, and more.Enterprises across many industries are already building with Anthropic models on Amazon Bedrock.LexisNexis Legal &amp; Professional, a leading global provider of information and analytics, is using a custom, fine-tuned Claude 2 model to deliver conversational search, insightful summarization, and intelligent legal drafting capabilities via the company’s new Lexis+ AI solution. Premier asset management firmBridgewater Associatesis developing an investment analyst assistant powered by Claude 2 to generate elaborate charts, compute financial indicators, and create summaries of the results.Lonely Planet, a renowned travel publisher, reduced its itinerary generation costs by almost 80 percent after deploying Claude 2; synthesizing its decades of travel content to deliver cohesive, highly accurate travel recommendations.Anthropic and Amazon are both committed to the safe training and deployment of advanced foundation models. Amazon is an industry leader in cloud security and, as part of this agreement, is committed to promoting and implementing safety best practices on Amazon Bedrock to ensure the responsible use of our products and services. Both companies are actively engaged across a number of organizations to promote the responsible development and deployment of AI technologies, including the Global Partnership on AI (GPAI), the Partnership on AI (PAI), and the National Institute of Standards and Technology (NIST). Most recently, in July, both Amazon and Anthropic independently supported a set of voluntary safety commitments led by the White House to ensure that the future of transformative AI is guided by safety, security, and trust.As part of the investment, Amazon will take a minority stake in Anthropic. Our corporate governance structure remains unchanged, with theLong Term Benefit Trustcontinuing to guide Anthropic in accordance with ourResponsible Scaling Policy. As outlined in this policy, we will conduct pre-deployment tests of new models to help us manage the risks of increasingly capable AI systems.Training state-of-the-art models requires extensive resources including compute power and research programs. Amazon’s investment and supply of AWS Trainium and Inferentia technology will ensure we’re equipped to continue advancing the frontier of AI safety and research. We look forward to working closely with Amazon to responsibly scale adoption of Claude and deliver safe AI cloud technologies to organizations around the world.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-amazon"/>
    <category term="news" label="News"/>
    <published>2023-09-25T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/amazon-bedrock-general-availability</id>
    <title>Claude on Amazon Bedrock now available to every AWS customer</title>
    <updated>2023-09-28T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Claude is now generally available onAmazon Bedrock, the fully managed service that provides Amazon Web Services (AWS) customers with secure cloud access to foundation models and tools for building generative AI applications.This means that every AWS customer can now build with Claude, and will soon gain access to an exciting roadmap of new experiences - includingAgents for Amazon Bedrock, which our team has been instrumental in developing.Currently available in preview, Agents for Amazon Bedrock can orchestrate and perform API calls using the popularAWS Lambdafunctions. Through this feature, Claude can take on a more expanded role as an agent to understand user requests, break down complex tasks into multiple steps, carry on conversations to collect additional details, look up information, and take actions to fulfill requests. For example, an e-commerce app that offers a chat assistant built with Claude can go beyond just querying product inventory – it can actually help customers update their orders, make exchanges, and look up relevant user manuals.We also recently shared that we’ll offersecure customization and fine-tuningof Claude models through the service. This technique optimizes Claude’s performance with AWS customers’ expert knowledge and proprietary data to drive more relevant results, while limiting the potential for harmful outputs.The general availability of Claude on Amazon Bedrock advances our work in helping enterprises responsibly integrate transformative AI. In the coming months, we’ll be announcing even more application layer solutions to help organizations get the most value out of Claude.Spotlight: How Bridgewater Associates is augmenting analyst productivity with Claude&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/amazon-bedrock-general-availability"/>
    <category term="news" label="News"/>
    <published>2023-09-28T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/uk-ai-safety-summit</id>
    <title>Dario Amodei’s prepared remarks from the AI Safety Summit on Anthropic’s Responsible Scaling Policy</title>
    <updated>2023-11-01T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Before I get into Anthropic’sResponsible Scaling Policy (RSP), it’s worth explaining some of the unique challenges around measuring AI risks that led us to develop our RSP. The most important thing to understand about AI is how quickly it is moving. A few years ago, AI systems could barely string together a coherent sentence. Today they can pass medical exams, write poetry, and tell jokes. This rapid progress is ultimately driven by the amount of available computation, which is growing by 8x per year and is unlikely to slow down in the next few years. Thegeneraltrend of rapid improvement is predictable, however, it is actually very difficult to predict when AI will acquirespecificskills or knowledge. This unfortunately includes dangerous skills, such as the ability to construct biological weapons1. We are thus facing a number of potential AI-related threats which, although relatively limited given today’s systems, are likely to become very serious at some unknown point in the near future. This is very different from most other industries: imagine if each new model of car had some chance of spontaneously sprouting a new (and dangerous) power, like the ability to fire a rocket boost or accelerate to supersonic speeds.We need both a way to frequently monitor these emerging risks, and a protocol for responding appropriately when they occur. Responsible scaling policies—initially suggested by the Alignment Research Center—attempt to meet this need. Anthropic published its RSP in September, and was the first major AI company to do so. It has two major components:&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/uk-ai-safety-summit"/>
    <category term="news" label="News"/>
    <published>2023-11-01T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/policy-recap-q4-2023</id>
    <title>Thoughts on the US Executive Order, G7 Code of Conduct, and Bletchley Park Summit</title>
    <updated>2023-11-05T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Three major events in AI policy happened in the last week: the US government issued a wide-rangingExecutive Order on AI, the G7 produced anInternational Code of Conduct, and the UK government held a first-of-its-kind summit on AI safety at Bletchley Park which produced theBletchley Declaration. In this post, we briefly summarize each of these events and what we believe they mean for AI policy.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/policy-recap-q4-2023"/>
    <category term="news" label="News"/>
    <published>2023-11-05T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-2-1</id>
    <title>Introducing Claude 2.1</title>
    <updated>2023-11-21T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Our latest model, Claude 2.1, is now available over API in our Console and is powering ourclaude.aichat experience. Claude 2.1 delivers advancements in key capabilities for enterprises—including an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and our new beta feature: tool use. We are also updating ourpricingto improve cost efficiency for our customers across models.200K Context WindowSince our launch earlier this year, Claude has been used by millions of people for a wide range of applications—from translating academic papers to drafting business plans and analyzing complex contracts. In discussions with our users, they’ve asked for larger context windows and more accurate outputs when working with long documents.In response, we’re doubling the amount of information you can relay to Claude with a limit of 200,000 tokens, translating to roughly 150,000 words, or over 500 pages of material. Our users can now upload technical documentation like entire codebases, financial statements like S-1s, or even long literary works like The Iliad or The Odyssey. By being able to talk to large bodies of content or data, Claude can summarize, perform Q&amp;A, forecast trends, compare and contrast multiple documents, and much more.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-2-1"/>
    <category term="news" label="News"/>
    <published>2023-11-21T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-2-1-prompting</id>
    <title>Long context prompting for Claude 2.1</title>
    <updated>2023-12-06T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We recently launchedClaude 2.1, our state-of-the-art model offering a 200K token context window - the equivalent of around 500 pages of information. Claude 2.1 excels at real-world retrieval tasks across longer contexts.Claude 2.1 was trained using large amounts of feedback on long document tasks that our users find valuable, like summarizing an S-1 length document. This data included real tasks performed on real documents, with Claude being trained to make fewer mistakes and to avoid expressing unsupported claims.Being trained on real-world, complex retrieval tasks is why Claude 2.1 shows a 30% reduction in incorrect answers compared with Claude 2.0, and a 3-4x lower rate of mistakenly stating that a document supports a claim when it does not.Additionally, Claude's memory is improved over these very long contexts:&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-2-1-prompting"/>
    <category term="news" label="News"/>
    <published>2023-12-06T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/expanded-legal-protections-api-improvements</id>
    <title>Expanded legal protections and improvements to our API</title>
    <updated>2023-12-19T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are introducing new, simplified Commercial Terms of Service with an expanded copyright indemnity, as well as an improved developer experience with our beta Messages API. Customers will now enjoy increased protection and peace of mind as they build with Claude, as well as a more streamlined API that is easier to use.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/expanded-legal-protections-api-improvements"/>
    <category term="news" label="News"/>
    <published>2023-12-19T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/preparing-for-global-elections-in-2024</id>
    <title>Preparing for global elections in 2024</title>
    <updated>2024-02-16T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Over half of the world’s population will vote this year with high profile elections taking place around the world, including in the United States, India, Europe, and many other countries and regions. At Anthropic, we’ve been preparing since last July for how our AI systems might be used during elections. In this post, we’ll discuss some of the specific steps we’ve taken to help us detect and mitigate potential misuse of our AI tools in political contexts.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/preparing-for-global-elections-in-2024"/>
    <category term="news" label="News"/>
    <published>2024-02-16T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/prompt-engineering-for-business-performance</id>
    <title>Prompt engineering for business performance</title>
    <updated>2024-02-29T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;As businesses build with generative AI models, crafting effective prompts has become critical for producing high-quality outputs. This post explains basic prompt engineering techniques that help our customers get the most value from Claude. With the right prompts, businesses can tap into the full potential of AI to increase productivity across a wide range of tasks.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/prompt-engineering-for-business-performance"/>
    <category term="news" label="News"/>
    <published>2024-02-29T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-3-family</id>
    <title>Introducing the next generation of Claude</title>
    <updated>2024-03-04T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus. Each successive model offers increasingly powerful performance, allowing users to select the optimal balance of intelligence, speed, andcostfor their specific application.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-3-family"/>
    <category term="news" label="News"/>
    <published>2024-03-04T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-3-haiku</id>
    <title>Claude 3 Haiku: our fastest model yet</title>
    <updated>2024-03-13T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we’re releasing Claude 3 Haiku, the fastest and most affordable model in its intelligence class. With state-of-the-art vision capabilities and strong performance on industry benchmarks, Haiku is a versatile solution for a wide range of enterprise applications. The model is now available alongside Sonnet and Opus in the Claude API and on claude.ai for our Claude Pro subscribers.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-3-haiku"/>
    <category term="news" label="News"/>
    <published>2024-03-13T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/google-vertex-general-availability</id>
    <title>Claude 3 models on Vertex AI</title>
    <updated>2024-03-19T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Claude 3 Haiku and Claude 3 Sonnet are now generally available on Google Cloud’s Vertex AI platform. Enterprises can use our state-of-the-art models that optimize intelligence, speed, and cost with Google Cloud's robust infrastructure and tools. This collaboration enables businesses to quickly prototype and scale generative AI solutions with enterprise-grade data privacy and security. The benefits for Google Cloud customers include the ability to keep their data within their existing cloud environment, simplify data governance, reduce operational costs and complexities, and more easily manage access permissions.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/google-vertex-general-availability"/>
    <category term="news" label="News"/>
    <published>2024-03-19T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/accenture-aws-anthropic</id>
    <title>Anthropic, AWS, and Accenture team up to build trusted solutions for enterprises</title>
    <updated>2024-03-20T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today weannounced a collaborationwith Amazon Web Services (AWS) and Accenture. All three organizations are providing key resources to take generative AI ideas from concept to production, especially those in regulated sectors where accuracy, reliability and data security are paramount. Enterprises will be able to deploy models to address their specific needs, while keeping their data private and secure.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/accenture-aws-anthropic"/>
    <category term="news" label="News"/>
    <published>2024-03-20T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/third-party-testing</id>
    <title>Third-party testing as a key ingredient of AI policy</title>
    <updated>2024-03-25T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We believe that the AI sector needs effective third-party testing for frontier AI systems. Developing a testing regime and associated policy interventions based on the insights of industry, government, and academia is the best way to avoid societal harm—whether deliberate or accidental—from AI systems.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/third-party-testing"/>
    <category term="news" label="News"/>
    <published>2024-03-25T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/child-safety-principles</id>
    <title>Aligning on child safety principles</title>
    <updated>2024-04-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Alongside other leading AI companies, we are committed to implementing robust child safety measures in the development, deployment, and maintenance of generative AI technologies. This new initiative, led byThorn, a nonprofit dedicated to defending children from sexual abuse, andAll Tech Is Human, an organization dedicated to collectively tackling tech and society's complex problems, aims to mitigate the risks generative AI poses to children.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/child-safety-principles"/>
    <category term="news" label="News"/>
    <published>2024-04-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/team-plan-and-ios</id>
    <title>Introducing the Claude Team plan and iOS app</title>
    <updated>2024-05-01T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re announcing two updates for Claude: a new Team plan and an iOS app.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/team-plan-and-ios"/>
    <category term="news" label="News"/>
    <published>2024-05-01T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/updating-our-usage-policy</id>
    <title>Updating our Usage Policy</title>
    <updated>2024-05-10T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/updating-our-usage-policy"/>
    <category term="news" label="News"/>
    <published>2024-05-10T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-europe</id>
    <title>Claude is now available in Europe</title>
    <updated>2024-05-14T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We’re excited to announce thatClaude, Anthropic’s trusted AI assistant, is now available for people and businesses across Europe to enhance their productivity and creativity. Starting today, they will be able to use:&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-europe"/>
    <category term="news" label="News"/>
    <published>2024-05-14T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/mike-krieger-joins-anthropic</id>
    <title>Mike Krieger joins Anthropic as Chief Product Officer</title>
    <updated>2024-05-15T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We're excited to announce that Mike Krieger has joined Anthropic as our Chief Product Officer. Mike will oversee Anthropic's product engineering, product management, and product design efforts as we work to expand our suite of enterprise applications and bring Claude to a wider audience.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/mike-krieger-joins-anthropic"/>
    <category term="news" label="News"/>
    <published>2024-05-15T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy</id>
    <title>Reflections on our Responsible Scaling Policy</title>
    <updated>2024-05-20T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Last summer we published our firstResponsible Scaling Policy (RSP), which focuses on addressing catastrophic safety failures and misuse of frontier models. In adopting this policy, our primary goal is to help turn high-level safety concepts into practical guidelines for fast-moving technical organizations and demonstrate their viability as possible standards. As we operationalize the policy, we expect to learn a great deal and plan to share our findings. This post shares reflections from implementing the policy so far. We are also working on an updated RSP and will share this soon.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/reflections-on-our-responsible-scaling-policy"/>
    <category term="news" label="News"/>
    <published>2024-05-20T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/prompt-generator</id>
    <title>Generate better prompts in the developer console</title>
    <updated>2024-05-20T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;You can now generate production-ready prompt templates in the Anthropic Console. Describe what you want to achieve, and Claude will use prompt engineering techniques such as chain-of-thought reasoning to create an effective, precise, and reliable prompt.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/prompt-generator"/>
    <category term="news" label="News"/>
    <published>2024-05-20T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/krishna-rao-joins-anthropic</id>
    <title>Krishna Rao joins Anthropic as Chief Financial Officer</title>
    <updated>2024-05-21T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We’re excited to announce that Krishna Rao has joined Anthropic as our Chief Financial Officer. With nearly 20 years of experience as a strategic finance leader for customer-centric, world-class brands and as an investor, Krishna will play a crucial role in shaping Anthropic's financial strategy and operations as we continue to build on our strong enterprise momentum and advance our international expansion.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/krishna-rao-joins-anthropic"/>
    <category term="news" label="News"/>
    <published>2024-05-21T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/golden-gate-claude</id>
    <title>Golden Gate Claude</title>
    <updated>2024-05-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;UPDATE: Golden Gate Claude was online for a 24-hour period as a research demo and is no longer available. If you'd like to find out more about our research on interpretability and the activation of features within Claude, please seethis postor ourfull research paper.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/golden-gate-claude"/>
    <category term="news" label="News"/>
    <published>2024-05-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/jay-kreps-appointed-to-board-of-directors</id>
    <title>Jay Kreps appointed to Anthropic's Board of Directors</title>
    <updated>2024-05-29T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're announcing that Jay Kreps, co-founder and CEO of Confluent, has joined Anthropic's Board of Directors. Jay's extensive experience in building and scaling highly successful tech companies will play an important role as Anthropic prepares for the next phase of growth. His deep expertise in data infrastructure and open-source software development will be particularly valuable as we build data-driven product experiences for our growing base of enterprise customers.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/jay-kreps-appointed-to-board-of-directors"/>
    <category term="news" label="News"/>
    <published>2024-05-29T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/tool-use-ga</id>
    <title>Claude can now use tools</title>
    <updated>2024-05-30T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Tool use, which enables Claude to interact with external tools and APIs, is now generally available across the entire Claude 3 model family on the Anthropic Messages API, Amazon Bedrock, and Google Cloud's Vertex AI. With tool use, Claude can perform tasks, manipulate data, and provide more dynamic—and accurate—responses.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/tool-use-ga"/>
    <category term="news" label="News"/>
    <published>2024-05-30T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/introducing-claude-to-canada</id>
    <title>Introducing Claude to Canada</title>
    <updated>2024-06-05T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Claudeis now available in Canada. Starting today, people and businesses across the country will be able to access Claude via:&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/introducing-claude-to-canada"/>
    <category term="news" label="News"/>
    <published>2024-06-05T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/testing-and-mitigating-elections-related-risks</id>
    <title>Testing and mitigating elections-related risks</title>
    <updated>2024-06-06T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;With global elections in 2024, we're often asked how we're safeguarding election integrity as AI evolves. This blog provides a snapshot of the work we've done since last summer to test our models for elections-related risks.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/testing-and-mitigating-elections-related-risks"/>
    <category term="news" label="News"/>
    <published>2024-06-06T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems</id>
    <title>Challenges in red teaming AI systems</title>
    <updated>2024-06-12T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;In this post we detail insights from a sample of red teaming approaches that we’ve used to test our AI systems. Through this practice, we’ve begun to gather empirical data about the appropriate tool to reach for in a given situation, and the associated benefits and challenges with each approach. We hope this post is helpful for other companies trying to red team their AI systems, policymakers curious about how red teaming works in practice, and organizations that want to red team AI technology.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems"/>
    <category term="news" label="News"/>
    <published>2024-06-12T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-3-5-sonnet</id>
    <title>Claude 3.5 Sonnet</title>
    <updated>2024-06-21T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re launching Claude 3.5 Sonnet—our first release in the forthcoming Claude 3.5 model family. Claude 3.5 Sonnet raises the industry bar for intelligence, outperforming competitor models and Claude 3 Opus on a wide range of evaluations, with the speed and cost of our mid-tier model, Claude 3 Sonnet.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-3-5-sonnet"/>
    <category term="news" label="News"/>
    <published>2024-06-21T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/projects</id>
    <title>Collaborate with Claude on Projects</title>
    <updated>2024-06-25T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Our vision for Claude has always been to create AI systems that work alongside people and meaningfully enhance their workflows. As a step in this direction,Claude.aiPro and Team users can now organize their chats into Projects, bringing together curated sets of knowledge and chat activity in one place—with the ability to make their best chats with Claude viewable by teammates. With this new functionality, Claude can enable idea generation, more strategic decision-making, and exceptional results.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/projects"/>
    <category term="news" label="News"/>
    <published>2024-06-25T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/expanding-access-to-claude-for-government</id>
    <title>Expanding access to Claude for government</title>
    <updated>2024-06-26T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Anthropic's mission is to build reliable, interpretable, steerable AI systems. We have been excited to see our technology used in areas like coding, customer service, drug discovery, and medical research. We're eager to make these tools available through expanded offerings to government users. Leveraging the flexibility and security of Amazon Web Services [AWS], our AI models Claude 3 Haiku and Claude 3 Sonnet are now available in the AWS Marketplace for the US Intelligence Community [IC] and in AWS GovCloud.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/expanding-access-to-claude-for-government"/>
    <category term="news" label="News"/>
    <published>2024-06-26T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations</id>
    <title>A new initiative for developing third-party model evaluations</title>
    <updated>2024-07-01T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Arobust, third-party evaluation ecosystemis essential for assessing AI capabilities and risks, but the current evaluations landscape is limited. Developing high-quality, safety-relevant evaluations remains challenging, and the demand is outpacing the supply. To address this, today we're introducing a new initiative to fund evaluations developed by third-party organizations that can effectively measure advanced capabilities in AI models. Our investment in these evaluations is intended to elevate the entire field of AI safety, providing valuable tools that benefit the whole ecosystem.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations"/>
    <category term="news" label="News"/>
    <published>2024-07-01T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/evaluate-prompts</id>
    <title>Evaluate prompts in the developer console</title>
    <updated>2024-07-09T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;When building AI-powered applications, prompt quality significantly impacts results. But crafting high quality prompts is challenging, requiring deep knowledge of your application's needs and expertise with large language models. To speed up development and improve outcomes, we've streamlined this process to make it easier for users to produce high quality prompts.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/evaluate-prompts"/>
    <category term="news" label="News"/>
    <published>2024-07-09T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/fine-tune-claude-3-haiku</id>
    <title>Fine-tune Claude 3 Haiku in Amazon Bedrock</title>
    <updated>2024-07-11T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Update: As of November 1, 2024, fine-tuning Claude 3 Haiku in Amazon Bedrock is generally available.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/fine-tune-claude-3-haiku"/>
    <category term="news" label="News"/>
    <published>2024-07-11T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/android-app</id>
    <title>Claude Android app</title>
    <updated>2024-07-16T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;The new Claude Android app brings the power of Claude—including our most powerful model, Claude 3.5 Sonnet—to Android users. The app is free and accessible with all plans, including Pro and Team.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/android-app"/>
    <category term="news" label="News"/>
    <published>2024-07-16T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-partners-with-menlo-ventures-to-launch-anthology-fund</id>
    <title>Anthropic partners with Menlo Ventures to launch Anthology Fund</title>
    <updated>2024-07-17T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We’re launching the Anthology Fund in partnership with Menlo Ventures to accelerate the development of groundbreaking AI applications. The Fund is a $100 million initiative financed by Menlo to support startups innovating broadly with Anthropic technology.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-partners-with-menlo-ventures-to-launch-anthology-fund"/>
    <category term="news" label="News"/>
    <published>2024-07-17T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-brazil</id>
    <title>Claude is now available in Brazil</title>
    <updated>2024-08-01T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Claude, Anthropic’s trusted AI assistant, is now available in Brazil. Starting today, consumers and businesses in Brazil will be able to access Claude via:&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-brazil"/>
    <category term="news" label="News"/>
    <published>2024-08-01T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/model-safety-bug-bounty</id>
    <title>Expanding our model safety bug bounty program</title>
    <updated>2024-08-08T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;The rapid progression of AI model capabilities demands an equally swift advancement in safety protocols. As we work on developing the next generation of our AI safeguarding systems, we’re expanding our bug bounty program to introduce a new initiative focused on finding flaws in the mitigations we use to prevent misuse of our models.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/model-safety-bug-bounty"/>
    <category term="news" label="News"/>
    <published>2024-08-08T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/prompt-caching</id>
    <title>Prompt caching with Claude</title>
    <updated>2024-08-14T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Update: As of December 17, 2024, prompt caching is Generally Available on the Anthropic API. Prompt caching is also available in preview in Amazon Bedrock and on Google Cloud’s Vertex AI.Prompt caching, which enables developers to cache frequently used context between API calls, is now available on the Anthropic API. With prompt caching, customers can provide Claude with more background knowledge and example outputs—all while reducing costs by up to 90% and latency by up to 85% for long prompts. Prompt caching is available today in public beta for Claude 3.5 Sonnet, Claude 3 Opus, and Claude 3 Haiku.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/prompt-caching"/>
    <category term="news" label="News"/>
    <published>2024-08-14T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/artifacts</id>
    <title>Artifacts are now generally available</title>
    <updated>2024-08-27T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re makingArtifactsavailable for allClaude.aiusers across our Free, Pro, and Team plans. And now, you can create and view Artifacts on ouriOSandAndroidapps.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/artifacts"/>
    <category term="news" label="News"/>
    <published>2024-08-27T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/salesforce-partnership</id>
    <title>Salesforce teams up with Anthropic to enhance Einstein capabilities with Claude</title>
    <updated>2024-09-03T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Salesforce customers can now selectClaude modelsfor AI-powered Salesforce applications and experiences built with the Salesforce Platform. The latest Claude models—Claude 3.5 Sonnet, Claude 3 Opus, and Claude 3 Haiku—are now available to Salesforce customers using Amazon Bedrock.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/salesforce-partnership"/>
    <category term="news" label="News"/>
    <published>2024-09-03T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-for-enterprise</id>
    <title>Claude for Enterprise</title>
    <updated>2024-09-04T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re announcing theClaude Enterprise planto help organizations securely collaborate with Claude using internal knowledge.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-for-enterprise"/>
    <category term="news" label="News"/>
    <published>2024-09-04T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/workspaces</id>
    <title>Workspaces in the Anthropic API Console</title>
    <updated>2024-09-10T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We're introducing Workspaces in the Anthropic API Console to help developers efficiently manage multiple Claude deployments. Workspaces are unique environments that enable you to organize resources, streamline access controls, and set custom spend and rate limits on a more granular level.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/workspaces"/>
    <category term="news" label="News"/>
    <published>2024-09-10T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/contextual-retrieval</id>
    <title>Introducing Contextual Retrieval</title>
    <updated>2024-09-19T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;For an AI model to be useful in specific contexts, it often needs access to background knowledge. For example, customer support chatbots need knowledge about the specific business they're being used for, and legal analyst bots need to know about a vast array of past cases.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/contextual-retrieval"/>
    <category term="news" label="News"/>
    <published>2024-09-19T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/fine-tune-claude-3-haiku-ga</id>
    <title>Fine-tuning for Claude 3 Haiku in Amazon Bedrock is now generally available</title>
    <updated>2024-09-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Fine-tuning for Claude 3 Haiku is now generally available inAmazon Bedrock. With fine-tuning, you can customize Claude 3 Haiku—our fastest and most cost-effective model—with your own data to increase model performance for specialized tasks.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/fine-tune-claude-3-haiku-ga"/>
    <category term="news" label="News"/>
    <published>2024-09-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/message-batches-api</id>
    <title>Introducing the Message Batches API</title>
    <updated>2024-10-08T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Update: As of December 17, 2024, the Message Batches API is Generally Available on the Anthropic API. Customers using Claude in Amazon Bedrock can use batch inference. Batch predictions is also available in preview on Google Cloud’s Vertex AI.We’re introducing a newMessage Batches API—a powerful, cost-effective way to process large volumes of queries asynchronously.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/message-batches-api"/>
    <category term="news" label="News"/>
    <published>2024-10-08T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/us-elections-readiness</id>
    <title>U.S. Elections Readiness</title>
    <updated>2024-10-08T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;2024 marks the first United States (U.S.) election cycle where generative AI tools are widely available. Since July 2023, we havetaken concrete stepstohelp detect and mitigateagainst the potential misuse of our tools and to direct users to authoritative election information. Ahead of federal, state, and local elections in the U.S. on November 5, 2024, we are sharing a summary of our work thus far.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/us-elections-readiness"/>
    <category term="news" label="News"/>
    <published>2024-10-08T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy</id>
    <title>Announcing our updated Responsible Scaling Policy</title>
    <updated>2024-10-15T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we are publishing a significant update to our Responsible Scaling Policy (RSP), the risk governance framework we use to mitigate potential catastrophic risks from frontier AI systems.This update introduces a more flexible and nuanced approach to assessing and managing AI risks while maintaining our commitment not to train or deploy models unless we have implemented adequate safeguards. Key improvements include new capability thresholds to indicate when we will upgrade our safeguards, refined processes for evaluating model capabilities and the adequacy of our safeguards (inspired bysafety case methodologies), and new measures for internal governance and external input. By learning from our implementation experiences and drawing on risk management practices used in other high-consequence industries, we aim to better prepare for the rapid pace of AI advancement.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy"/>
    <category term="news" label="News"/>
    <published>2024-10-15T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/3-5-models-and-computer-use</id>
    <title>Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku</title>
    <updated>2024-10-22T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Update (12/03/2024): We have revised the pricing for Claude 3.5 Haiku. The model is now priced at $0.80 MTok input / $4 MTok output.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/3-5-models-and-computer-use"/>
    <category term="news" label="News"/>
    <published>2024-10-22T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/developing-computer-use</id>
    <title>Developing a computer use model</title>
    <updated>2024-10-22T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Claude can now use computers. Thelatest version of Claude 3.5 Sonnetcan, when run through the appropriate software setup, follow a user’s commands to move a cursor around their computer’s screen, click on relevant locations, and input information via a virtual keyboard, emulating the way people interact with their own computer.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/developing-computer-use"/>
    <category term="news" label="News"/>
    <published>2024-10-22T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/analysis-tool</id>
    <title>Introducing the analysis tool in Claude.ai</title>
    <updated>2024-10-24T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We're introducing the analysis tool, a new built-in feature forClaude.aithat enables Claude to write and run JavaScript code. Claude can now process data, conduct analysis, and produce real-time insights. The analysis tool is available for all Claude.ai users infeature preview.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/analysis-tool"/>
    <category term="news" label="News"/>
    <published>2024-10-24T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/github-copilot</id>
    <title>Claude 3.5 Sonnet on GitHub Copilot</title>
    <updated>2024-10-29T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Starting today, the newClaude 3.5 Sonnetbegins rolling out on GitHub Copilot, enabling developers to choose Claude 3.5 Sonnet for coding—directly in Visual Studio Code and GitHub.com. This integration brings Claude’s coding capabilities to GitHub’s community of over 100 million developers.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/github-copilot"/>
    <category term="news" label="News"/>
    <published>2024-10-29T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/the-case-for-targeted-regulation</id>
    <title>The case for targeted regulation</title>
    <updated>2024-10-31T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Increasingly powerful AI systems have the potential toaccelerate scientific progress, unlock new medical treatments, and grow the economy. But along with the remarkable new capabilities of these AIs come significant risks.Governments should urgently take action on AI policy in the next eighteen months. The window for proactive risk prevention is closing fast.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/the-case-for-targeted-regulation"/>
    <category term="news" label="News"/>
    <published>2024-10-31T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/prompt-improver</id>
    <title>Improve your prompts in the developer console</title>
    <updated>2024-11-14T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're introducing the ability to improve prompts and manage examples directly in theAnthropic Console. These features make it easier to leverage prompt engineering best practices and build more reliable AI applications.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/prompt-improver"/>
    <category term="news" label="News"/>
    <published>2024-11-14T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-amazon-trainium</id>
    <title>Powering the next generation of AI development with AWS</title>
    <updated>2024-11-22T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we’re announcing an expansion of our collaboration with Amazon Web Services (AWS), deepening our work together to develop and deploy advanced AI systems. This expanded partnership includes a new $4 billion investment from Amazon and establishes AWS as our primary cloud and training partner. This will bring Amazon's total investment in Anthropic to $8 billion, while maintaining their position as a minority investor.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-amazon-trainium"/>
    <category term="news" label="News"/>
    <published>2024-11-22T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/model-context-protocol</id>
    <title>Introducing the Model Context Protocol</title>
    <updated>2024-11-25T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're open-sourcing theModel Context Protocol(MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/model-context-protocol"/>
    <category term="news" label="News"/>
    <published>2024-11-25T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/styles</id>
    <title>Tailor Claude’s responses to your personal style</title>
    <updated>2024-11-26T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're announcing custom styles for allClaude.aiusers. Now you can tailor Claude's responses to your unique needs and workflows.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/styles"/>
    <category term="news" label="News"/>
    <published>2024-11-26T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/trainium2-and-distillation</id>
    <title>Claude 3.5 Haiku on AWS Trainium2 and model distillation in Amazon Bedrock</title>
    <updated>2024-12-03T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;As part of our expandedcollaboration with AWS, we’ve begun optimizing Claude models to run onAWS Trainium2, their most advanced AI chip.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/trainium2-and-distillation"/>
    <category term="news" label="News"/>
    <published>2024-12-03T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/elections-ai-2024</id>
    <title>Elections and AI in 2024: observations and learnings</title>
    <updated>2024-12-12T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;2024 marked the first major election cycle with widespread access to generative AI and the first major election year that Claude has been available. With concerns about generative AI's impact on election outcomes, we implemented proactive safety measures and drew upon usage analysis from our newClio tool. Across our products (Claude.ai, first party and third party API), election-related activity constituted less than 0.5% of overall use, ticking up to just over 1% of total usage in the weeks leading up to the US election. Below are insights about our election safety work and lessons learned for future elections.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/elections-ai-2024"/>
    <category term="news" label="News"/>
    <published>2024-12-12T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-achieves-iso-42001-certification-for-responsible-ai</id>
    <title>Anthropic achieves ISO 42001 certification for responsible AI</title>
    <updated>2025-01-13T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are excited to announce that Anthropic has achieved accredited certification under the newISO/IEC 42001:2023 standardfor our AI management system. ISO 42001 is the first international standard outlining requirements for AI governance and helps ensure AI systems are developed and used responsibly.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-achieves-iso-42001-certification-for-responsible-ai"/>
    <category term="news" label="News"/>
    <published>2025-01-13T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/introducing-citations-api</id>
    <title>Introducing Citations on the Anthropic API</title>
    <updated>2025-01-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're launching Citations, a new API feature that lets Claude ground its answers in source documents. Claude can now provide detailed references to the exact sentences and passages it uses to generate responses, leading to more verifiable, trustworthy outputs.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/introducing-citations-api"/>
    <category term="news" label="News"/>
    <published>2025-01-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/lyft-announcement</id>
    <title>Lyft to bring Claude to more than 40 million riders and over 1 million drivers</title>
    <updated>2025-02-06T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Lyft is working with Anthropic to introduce customer-first, AI-powered Lyft products. This is to enhance the rideshare experience for its community of more than 40 million annual riders and over 1 million drivers. The work includes early research testing of new models and technology, alongside initiatives to advance Lyft's engineering capabilities.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/lyft-announcement"/>
    <category term="news" label="News"/>
    <published>2025-02-06T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/the-anthropic-economic-index</id>
    <title>The Anthropic Economic Index</title>
    <updated>2025-02-10T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;In the coming years, AI systems will have a major impact on the ways people work. For that reason, we're launching theAnthropic Economic Index, an initiative aimed at understanding AI's effects on labor markets and the economy over time.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/the-anthropic-economic-index"/>
    <category term="news" label="News"/>
    <published>2025-02-10T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/paris-ai-summit</id>
    <title>Statement from Dario Amodei on the Paris AI Action Summit</title>
    <updated>2025-02-11T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We were pleased to attend the AI Action Summit in Paris, and we appreciate the French government’s efforts to bring together AI companies, researchers, and policymakers from across the world. We share the goal of responsibly advancing AI for the benefit of humanity. However, greater focus and urgency is needed on several topics given the pace at which the technology is progressing. The need for democracies to keep the lead, the risks of AI, and the economic transitions that are fast approaching—these should all be central features of the next summit.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/paris-ai-summit"/>
    <category term="news" label="News"/>
    <published>2025-02-11T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/mou-uk-government</id>
    <title>Anthropic signs MOU with UK Government to explore how AI can transform UK public services</title>
    <updated>2025-02-14T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Artificial Intelligence has the potential to transform how governments operate, driving efficiency and accessibility and revolutionizing the delivery of digital services to citizens worldwide.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/mou-uk-government"/>
    <category term="news" label="News"/>
    <published>2025-02-14T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-3-7-sonnet</id>
    <title>Claude 3.7 Sonnet and Claude Code</title>
    <updated>2025-02-24T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is madevisible to the user. API users also have fine-grained control overhow longthe model can think for.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-3-7-sonnet"/>
    <category term="news" label="News"/>
    <published>2025-02-24T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-and-alexa-plus</id>
    <title>Claude and Alexa+</title>
    <updated>2025-02-26T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're announcing that Claude models are helping power Alexa+. This collaboration is part of our ongoing partnership with Amazon to deliver advanced AI technology to businesses and consumers around the world.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-and-alexa-plus"/>
    <category term="news" label="News"/>
    <published>2025-02-26T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/introducing-anthropic-transparency-hub</id>
    <title>Introducing Anthropic's Transparency Hub</title>
    <updated>2025-02-27T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're launching Anthropic'sTransparency Hub—a detailed overview of concrete measures we're implementing to ensure our systems are safe, beneficial, and trustworthy.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/introducing-anthropic-transparency-hub"/>
    <category term="news" label="News"/>
    <published>2025-02-27T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-partners-with-u-s-national-labs-for-first-1-000-scientist-ai-jam</id>
    <title>Anthropic partners with U.S. National Labs for first 1,000 Scientist AI Jam</title>
    <updated>2025-02-28T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are proud to participate in the U.S. Department of Energy’s (DOE) first-ever 1,000 Scientist AI Jam, which will bring together scientists across multiple National Laboratories to evaluate frontier AI models on scientific research and national security applications. AI has the potential to dramatically accelerate scientific discovery and technological development,compressing decadesof scientific progress into just a few years by enabling a new era of invention and problem-solving that addresses humanity’s greatest challenges. This week, welaunchedClaude 3.7 Sonnet, the first hybrid reasoning model on the market, which will be evaluated at the event.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-partners-with-u-s-national-labs-for-first-1-000-scientist-ai-jam"/>
    <category term="news" label="News"/>
    <published>2025-02-28T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation</id>
    <title>Anthropic raises Series E at $61.5B post-money valuation</title>
    <updated>2025-03-03T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Anthropic has raised $3.5 billion at a $61.5 billion post-money valuation. The round was led by Lightspeed Venture Partners, with participation from Bessemer Venture Partners, Cisco Investments, D1 Capital Partners, Fidelity Management &amp; Research Company, General Catalyst, Jane Street, Menlo Ventures and Salesforce Ventures, among other new and existing investors.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-raises-series-e-at-usd61-5b-post-money-valuation"/>
    <category term="news" label="News"/>
    <published>2025-03-03T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan</id>
    <title>Anthropic’s Recommendations to OSTP for the U.S. AI Action Plan</title>
    <updated>2025-03-06T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;In response to the White House’sRequest for Information on an AI Action Plan, Anthropichas submitted recommendationsto the Office of Science and Technology Policy (OSTP). Our recommendations are designed to better prepare America to capture the economic benefits and national security implications of powerful AI systems.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan"/>
    <category term="news" label="News"/>
    <published>2025-03-06T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/upgraded-anthropic-console</id>
    <title>Get to production faster with the upgraded Anthropic Console</title>
    <updated>2025-03-06T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We've redesigned the Anthropic Console to serve as one place to build, test, and iterate on your AI deployment with Claude and your teammates.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/upgraded-anthropic-console"/>
    <category term="news" label="News"/>
    <published>2025-03-06T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/token-saving-updates</id>
    <title>Token-saving updates on the Anthropic API</title>
    <updated>2025-03-13T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We've made several updates to the Anthropic API that let developers significantly increase throughput and reduce token usage with Claude 3.7 Sonnet. These include: cache-aware rate limits, simpler prompt caching, and token-efficient tool use.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/token-saving-updates"/>
    <category term="news" label="News"/>
    <published>2025-03-13T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team</id>
    <title>Progress from our Frontier Red Team</title>
    <updated>2025-03-19T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;In this post, we are sharing what we have learned about the trajectory of potential national security risks from frontier AI models, along with some of our thoughts about challenges and best practices in evaluating these risks. The information in this post is based on work we’ve carried out over the last year across four model releases. Our assessment is that AI models are displaying ‘early warning’ signs of rapid progress in key dual-use capabilities: models are approaching, and in some cases exceeding, undergraduate-level skills in cybersecurity and expert-level knowledge in some areas of biology. However, present-day models fall short of thresholds at which we consider them to generate substantially elevated risks to national security.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team"/>
    <category term="news" label="News"/>
    <published>2025-03-19T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-s-response-to-governor-newsom-s-ai-working-group-draft-report</id>
    <title>Anthropic’s Response to Governor Newsom’s AI Working Group Draft Report</title>
    <updated>2025-03-19T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;This week, the California Governor’sWorking Group on AI Frontier Modelsreleased its draft report. We agree with the working group’s focus on the need forobjective standardsand evidence-based policy guidance, and especially its emphasis on transparency as a means to create a well functioning AI policy environment.When done thoughtfully, transparency can be a low-cost, high-impact means of growing the evidence base around a new technology, increasing consumer trust, and causing companies to enter into positive-sum competitions with one another. We welcome greater discussion of how frontier labs should be transparent about their AI development practices and were glad to see the working group emphasize this - in particular, we appreciated the focus on the need for labs to disclose how they secure their models from theft, and how they test their models for potential national security risks.Many of the report’s recommendations already reflect industry best practices which Anthropic adheres to: for example Anthropic’sResponsible Scaling Policypublicly lays out how we assess our models for misuse and autonomy risks and thresholds that trigger increased safety and security measures for us. We also publicly describe the results of our safety and security testing as part of each major model release, and performthird-party testingto augment our own internal tests. Many other frontier AI companies have similar practices.In line with the report’s findings, we believe governments could play a constructive role in improving transparency in the safety and security practices of frontier AI companies. At present frontier AI companies are notrequiredto have a safety and security policy (even one entirely of their choice), nor to describe it publicly, nor to publicly document the tests they run – and therefore not all companies do. We believe this could be done in alight-touch waythat does not impede innovation. As we wrote in ourrecent policy submission to the White House, we believe powerful AI systems will arrive soon - perhaps as early as the end of 2026 - so it is important we all devote effort to building a policy regime that creates greater transparency about the safety and security protocols of how AI systems are built.The Working Group has also highlighted areas where academia, civil society, and industry will need to apply more focus in the coming years - particularly on the economic impacts of AI, where Anthropic is today trying to contribute viaour Economic Index. We look forward to providing further feedback to the working group to aid and inform the work of finalizing the report. We commend the Governor for his foresight in kicking off this conversation, and we look forward to helping shape California’s approach to frontier model safety.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-s-response-to-governor-newsom-s-ai-working-group-draft-report"/>
    <category term="news" label="News"/>
    <published>2025-03-19T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/web-search</id>
    <title>Claude can now search the web</title>
    <updated>2025-03-20T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;You can now use Claude to search the internet to provide more up-to-date and relevant responses. With web search, Claude has access to the latest events and information, boosting its accuracy on tasks that benefit from the most recent data.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/web-search"/>
    <category term="news" label="News"/>
    <published>2025-03-20T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7</id>
    <title>Anthropic Economic Index: Insights from Claude 3.7 Sonnet</title>
    <updated>2025-03-27T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Last month, we launched theAnthropic Economic Index—a new initiative where we’re regularly releasing data and research aimed at understanding AI's effects on labor markets and the economy over time.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-economic-index-insights-from-claude-sonnet-3-7"/>
    <category term="news" label="News"/>
    <published>2025-03-27T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/introducing-claude-for-education</id>
    <title>Introducing Claude for Education</title>
    <updated>2025-04-02T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we're launching Claude for Education, a specialized version of Claude tailored for higher education institutions. This initiative equips universities to develop and implement AI-enabled approaches across teaching, learning, and administration—ensuring educators and students play a key role in actively shaping AI's role in society.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/introducing-claude-for-education"/>
    <category term="news" label="News"/>
    <published>2025-04-02T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-on-google-cloud-fedramp-high</id>
    <title>Claude on Google Cloud’s Vertex AI: FedRAMP High and IL2 Authorized</title>
    <updated>2025-04-02T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're announcing that Claude models are now authorized for FedRAMP High and IL-2 workloads through Google Cloud's Vertex AI platform.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-on-google-cloud-fedramp-high"/>
    <category term="news" label="News"/>
    <published>2025-04-02T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/Introducing-code-with-claude</id>
    <title>Introducing Anthropic's first developer conference: Code with Claude</title>
    <updated>2025-04-03T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;&lt;em&gt;No summary available&lt;/em&gt;&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/Introducing-code-with-claude"/>
    <category term="news" label="News"/>
    <published>2025-04-03T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/head-of-EMEA-new-roles</id>
    <title>Anthropic Appoints Guillaume Princen as Head of EMEA and Announces 100+ New Roles Across the Region</title>
    <updated>2025-04-08T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;&lt;em&gt;No summary available&lt;/em&gt;&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/head-of-EMEA-new-roles"/>
    <category term="news" label="News"/>
    <published>2025-04-08T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude</id>
    <title>Anthropic Education Report: How University Students Use Claude</title>
    <updated>2025-04-08T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;AI systems are no longer just specialized research tools: they’re everyday academic companions. As AIs integrate more deeply into educational environments, we need to consider important questions about learning, assessment, and skill development. Until now, most discussions have relied on surveys and controlled experiments rather than direct evidence of how students naturally integrate AI into their academic work in real settings.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/anthropic-education-report-how-university-students-use-claude"/>
    <category term="news" label="News"/>
    <published>2025-04-08T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/max-plan</id>
    <title>Introducing the Max Plan</title>
    <updated>2025-04-09T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Four major upgrades to the Max plan&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/max-plan"/>
    <category term="news" label="News"/>
    <published>2025-04-09T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/research</id>
    <title>Claude takes research to new places</title>
    <updated>2025-04-15T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Earlier this year, we introducedour visionfor Claude as your collaborative partner that delivers hours of work in minutes. In line with this vision, we’re continuing to expand the context that Claude has access to, as well as the kinds of outputs you can generate with Claude to support your personal and professional work.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/research"/>
    <category term="news" label="News"/>
    <published>2025-04-15T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/our-approach-to-understanding-and-addressing-ai-harms</id>
    <title>Our Approach to Understanding and Addressing AI Harms</title>
    <updated>2025-04-21T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;As AI capabilities rapidly advance, understanding and addressing the full spectrum of potential impacts becomes increasingly important. Today, we're sharing insights into our evolving approach to assessing and mitigating various harms that could result from our systems, ranging from catastrophic scenarios like biological threats to critical concerns like child safety, disinformation and fraud.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/our-approach-to-understanding-and-addressing-ai-harms"/>
    <category term="news" label="News"/>
    <published>2025-04-21T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/detecting-and-countering-malicious-uses-of-claude-march-2025</id>
    <title>Detecting and Countering Malicious Uses of Claude: March 2025</title>
    <updated>2025-04-23T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We are committed to preventing misuse of our Claude models by adversarial actors while maintaining their utility for legitimate users. While our safety measures successfully prevent many harmful outputs, threat actors continue to explore methods to circumvent these protections. We are continuously using learnings to upgrade our safeguards.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/detecting-and-countering-malicious-uses-of-claude-march-2025"/>
    <category term="news" label="News"/>
    <published>2025-04-23T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/introducing-the-anthropic-economic-advisory-council</id>
    <title>Introducing the Anthropic Economic Advisory Council</title>
    <updated>2025-04-28T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re announcing the formation of the Anthropic Economic Advisory Council, a group of distinguished economists who will provide Anthropic with expert guidance on the economic implications of AI development and deployment. The Council will advise Anthropic on AI's impact on labor markets, economic growth, and broader socioeconomic systems. This work will inform the research agenda for theAnthropic Economic Index, an initiative that aims to understand AI’s impact on the labor market and global economy over time.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/introducing-the-anthropic-economic-advisory-council"/>
    <category term="news" label="News"/>
    <published>2025-04-28T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/securing-america-s-compute-advantage-anthropic-s-position-on-the-diffusion-rule</id>
    <title>Securing America's Compute Advantage: Anthropic’s Position on the Diffusion Rule</title>
    <updated>2025-04-30T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;In response to the Department of Commerce's "Framework for Artificial Intelligence Diffusion" interim final rule, Anthropic hassubmitted detailed analysis and recommendationsfor maintaining and strengthening export controls on advanced semiconductors. At the heart of our recommendations is a clear message: maintaining America's compute advantage through export controls is essential for national security and economic prosperity as powerful new AI systems are developed in the coming years.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/securing-america-s-compute-advantage-anthropic-s-position-on-the-diffusion-rule"/>
    <category term="news" label="News"/>
    <published>2025-04-30T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/integrations</id>
    <title>Claude can now connect to your world</title>
    <updated>2025-05-01T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we're announcing Integrations, a new way to connect your apps and tools to Claude. We're also expanding Claude'sResearchcapabilities with an advanced mode that searches the web, your Google Workspace, and now your Integrations too. Claude can research for up to 45 minutes before delivering a comprehensive report, complete with citations. In addition to these updates, we're making web search available globally for all Claude users on paid plans.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/integrations"/>
    <category term="news" label="News"/>
    <published>2025-05-01T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/ai-for-science-program</id>
    <title>Introducing Anthropic's AI for Science Program</title>
    <updated>2025-05-05T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re launching Anthropic's AI for Science program – a new initiative designed to accelerate scientific research and discovery through access to our API. This program will provide free API credits to support researchers working on high-impact scientific projects, with a particular focus on biology and life sciences applications.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/ai-for-science-program"/>
    <category term="news" label="News"/>
    <published>2025-05-05T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/web-search-api</id>
    <title>Introducing web search on the Anthropic API</title>
    <updated>2025-05-07T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're introducing web search on the Anthropic API—a new tool that gives Claude access to current information from across the web. With web search enabled, developers can build Claude-powered applications and agents that deliver up-to-date insights.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/web-search-api"/>
    <category term="news" label="News"/>
    <published>2025-05-07T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/testing-our-safety-defenses-with-a-new-bug-bounty-program</id>
    <title>Testing our safety defenses with a new bug bounty program</title>
    <updated>2025-05-14T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're launching a new bug bounty program to stress-test our latest safety measures. Similar to the program we announced lastsummer, we're challenging researchers to find universal jailbreaks in safety classifiers that we haven't yet deployed publicly. These safeguards are part of the advanced protections we’ve developed to help us meet the AI Safety Level-3 (ASL-3) Deployment Standard as part of ourResponsible Scaling Policy, the framework that governs how we develop and deploy increasingly capable AI models safely.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/testing-our-safety-defenses-with-a-new-bug-bounty-program"/>
    <category term="news" label="News"/>
    <published>2025-05-14T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/agent-capabilities-api</id>
    <title>New capabilities for building agents on the Anthropic API</title>
    <updated>2025-05-22T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we're announcing four new capabilities on the Anthropic API that enable developers to build more powerful AI agents: the code execution tool, MCP connector, Files API, and the ability to cache prompts for up to one hour.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/agent-capabilities-api"/>
    <category term="news" label="News"/>
    <published>2025-05-22T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-4</id>
    <title>Introducing Claude 4</title>
    <updated>2025-05-22T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today, we’re introducing the next generation of Claude models:Claude Opus 4andClaude Sonnet 4, setting new standards for coding, advanced reasoning, and AI agents.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-4"/>
    <category term="news" label="News"/>
    <published>2025-05-22T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/activating-asl3-protections</id>
    <title>Activating AI Safety Level 3 Protections</title>
    <updated>2025-05-22T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We have activated the AI Safety Level 3 (ASL-3) Deployment and Security Standards described in Anthropic’s Responsible Scaling Policy (RSP) in conjunction with launching Claude Opus 4. The ASL-3 Security Standard involves increased internal security measures that make it harder to steal model weights, while the corresponding Deployment Standard covers a narrowly targeted set of deployment measures designed to limit the risk of Claude being misused specifically for the development or acquisition of chemical, biological, radiological, and nuclear (CBRN) weapons. These measures should not lead Claude to refuse queries except on a very narrow set of topics.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/activating-asl3-protections"/>
    <category term="news" label="News"/>
    <published>2025-05-22T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/reed-hastings</id>
    <title>Reed Hastings appointed to Anthropic’s board of directors</title>
    <updated>2025-05-28T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Today we announced that Reed Hastings, Chairman and co-founder of Netflix who served as its CEO for over 25 years, has been appointed to Anthropic's board of directors by our Long Term Benefit Trust. Hastings brings extensive experience from founding and scaling Netflix into a global entertainment powerhouse, along with his service on the boards of Facebook, Microsoft, and Bloomberg.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/reed-hastings"/>
    <category term="news" label="News"/>
    <published>2025-05-28T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers</id>
    <title>Claude Gov Models for U.S. National Security Customers</title>
    <updated>2025-06-06T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;We’re introducing a custom set of Claude Gov models built exclusively for U.S. national security customers. The models are already deployed by agencies at the highest level of U.S. national security, and access to these models is limited to those who operate in such classified environments.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/claude-gov-models-for-u-s-national-security-customers"/>
    <category term="news" label="News"/>
    <published>2025-06-06T12:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://www.anthropic.com/news/national-security-expert-richard-fontaine-appointed-to-anthropic-s-long-term-benefit-trust</id>
    <title>National Security Expert Richard Fontaine appointed to Anthropic’s Long-Term Benefit Trust</title>
    <updated>2025-06-07T12:00:00+00:00</updated>
    <content type="html">&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt; news&lt;/p&gt;&lt;p&gt;Anthropic's Long-Term Benefit Trust today announced the appointment of Richard Fontaine, CEO of the Center for a New American Security, as a new member of the Trust. The Long-Term Benefit Trust (LTBT) is an independent body designed to help Anthropic achieve its public benefit mission.&lt;/p&gt;</content>
    <link href="https://www.anthropic.com/news/national-security-expert-richard-fontaine-appointed-to-anthropic-s-long-term-benefit-trust"/>
    <category term="news" label="News"/>
    <published>2025-06-07T12:00:00+00:00</published>
  </entry>
</feed>
